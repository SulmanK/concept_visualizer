name: Deploy Backend and Update Frontend Monitor

on:
  workflow_run:
    workflows: ["CI Tests & Deployment"]
    types:
      - completed
    branches: [develop, main]

jobs:
  deploy:
    name: Deploy to GCP
    # Only run if the CI workflow succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha || github.ref }}

      # Set environment-specific variables
      - name: Set Environment Specifics
        id: set_env
        run: |
          if [[ "${{ github.event.workflow_run.head_branch || github.ref_name }}" == "develop" ]]; then
            echo "ENVIRONMENT=dev" >> $GITHUB_ENV
            echo "GCP_PROJECT_ID=${{ secrets.DEV_GCP_PROJECT_ID }}" >> $GITHUB_ENV
            echo "GCP_ZONE=${{ secrets.DEV_GCP_ZONE }}" >> $GITHUB_ENV
            echo "NAMING_PREFIX=${{ secrets.DEV_NAMING_PREFIX }}" >> $GITHUB_ENV
            echo "WORKLOAD_IDENTITY_PROVIDER=${{ secrets.DEV_WORKLOAD_IDENTITY_PROVIDER }}" >> $GITHUB_ENV
            echo "CICD_SERVICE_ACCOUNT=${{ secrets.DEV_CICD_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "ARTIFACT_REGISTRY_REPO_NAME=${{ secrets.DEV_ARTIFACT_REGISTRY_REPO_NAME }}" >> $GITHUB_ENV
            echo "WORKER_SERVICE_ACCOUNT_EMAIL=${{ secrets.DEV_WORKER_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "API_SERVICE_ACCOUNT_EMAIL=${{ secrets.DEV_API_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "WORKER_MIN_INSTANCES=${{ secrets.DEV_WORKER_MIN_INSTANCES || 0 }}" >> $GITHUB_ENV
            echo "WORKER_MAX_INSTANCES=${{ secrets.DEV_WORKER_MAX_INSTANCES || 5 }}" >> $GITHUB_ENV
            echo "FRONTEND_UPTIME_CHECK_ID=${{ secrets.DEV_FRONTEND_UPTIME_CHECK_CONFIG_ID }}" >> $GITHUB_ENV
            echo "VERCEL_PROJECT_ID=${{ secrets.VERCEL_PROJECT_ID_DEV }}" >> $GITHUB_ENV
          elif [[ "${{ github.event.workflow_run.head_branch || github.ref_name }}" == "main" ]]; then
            echo "ENVIRONMENT=prod" >> $GITHUB_ENV
            echo "GCP_PROJECT_ID=${{ secrets.PROD_GCP_PROJECT_ID }}" >> $GITHUB_ENV
            echo "GCP_ZONE=${{ secrets.PROD_GCP_ZONE }}" >> $GITHUB_ENV
            echo "NAMING_PREFIX=${{ secrets.PROD_NAMING_PREFIX }}" >> $GITHUB_ENV
            echo "WORKLOAD_IDENTITY_PROVIDER=${{ secrets.PROD_WORKLOAD_IDENTITY_PROVIDER }}" >> $GITHUB_ENV
            echo "CICD_SERVICE_ACCOUNT=${{ secrets.PROD_CICD_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "ARTIFACT_REGISTRY_REPO_NAME=${{ secrets.PROD_ARTIFACT_REGISTRY_REPO_NAME }}" >> $GITHUB_ENV
            echo "WORKER_SERVICE_ACCOUNT_EMAIL=${{ secrets.PROD_WORKER_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "API_SERVICE_ACCOUNT_EMAIL=${{ secrets.PROD_API_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "WORKER_MIN_INSTANCES=${{ secrets.PROD_WORKER_MIN_INSTANCES || 0 }}" >> $GITHUB_ENV
            echo "WORKER_MAX_INSTANCES=${{ secrets.PROD_WORKER_MAX_INSTANCES || 5 }}" >> $GITHUB_ENV
            echo "FRONTEND_UPTIME_CHECK_ID=${{ secrets.PROD_FRONTEND_UPTIME_CHECK_CONFIG_ID }}" >> $GITHUB_ENV
            echo "VERCEL_PROJECT_ID=${{ secrets.VERCEL_PROJECT_ID_PROD }}" >> $GITHUB_ENV
          else
            echo "Branch is not develop or main, skipping deployment."
            exit 0
          fi
          echo "REGION=${{ secrets.GCP_REGION }}" >> $GITHUB_ENV

      # Authenticate with the appropriate environment credentials
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ env.CICD_SERVICE_ACCOUNT }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18

      - name: Install dependencies
        run: |
          npm install -g vercel
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Deploy Cloud Function Worker
        run: |
          FUNCTION_NAME="${{ env.NAMING_PREFIX }}-worker-${{ env.ENVIRONMENT }}"
          TASKS_TOPIC_NAME="${{ env.NAMING_PREFIX }}-tasks-${{ env.ENVIRONMENT }}"

          echo "Deploying Function: $FUNCTION_NAME"
          echo "Trigger Topic: $TASKS_TOPIC_NAME"
          echo "Service Account: ${{ env.WORKER_SERVICE_ACCOUNT_EMAIL }}"

          # Construct --set-secrets argument dynamically
          SECRETS_ARG=""
          SECRETS_ARG+="CONCEPT_SUPABASE_URL=${{ env.NAMING_PREFIX }}-supabase-url-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_SUPABASE_KEY=${{ env.NAMING_PREFIX }}-supabase-key-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_SUPABASE_SERVICE_ROLE=${{ env.NAMING_PREFIX }}-supabase-service-role-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_SUPABASE_JWT_SECRET=${{ env.NAMING_PREFIX }}-supabase-jwt-secret-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_JIGSAWSTACK_API_KEY=${{ env.NAMING_PREFIX }}-jigsawstack-api-key-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_UPSTASH_REDIS_ENDPOINT=${{ env.NAMING_PREFIX }}-redis-endpoint-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_UPSTASH_REDIS_PASSWORD=${{ env.NAMING_PREFIX }}-redis-password-${{ env.ENVIRONMENT }}:latest"

          # Deploy the function using the 'backend' directory as source with the shim main.py
          gcloud functions deploy "$FUNCTION_NAME" \
            --gen2 \
            --region="${{ env.REGION }}" \
            --project="${{ env.GCP_PROJECT_ID }}" \
            --runtime=python311 \
            --entry-point=handle_pubsub \
            --run-service-account="${{ env.WORKER_SERVICE_ACCOUNT_EMAIL }}" \
            --trigger-topic="$TASKS_TOPIC_NAME" \
            --timeout=540s \
            --memory=2048Mi \
            --min-instances=${{ env.WORKER_MIN_INSTANCES }} \
            --max-instances=${{ env.WORKER_MAX_INSTANCES }} \
            --set-env-vars="ENVIRONMENT=${{ env.ENVIRONMENT }},GCP_PROJECT_ID=${{ env.GCP_PROJECT_ID }},CONCEPT_STORAGE_BUCKET_CONCEPT=concept-images-${{ env.ENVIRONMENT }},CONCEPT_STORAGE_BUCKET_PALETTE=palette-images-${{ env.ENVIRONMENT }},CONCEPT_DB_TABLE_TASKS=tasks_${{ env.ENVIRONMENT }},CONCEPT_DB_TABLE_CONCEPTS=concepts_${{ env.ENVIRONMENT }},CONCEPT_DB_TABLE_PALETTES=color_variations_${{ env.ENVIRONMENT }},CONCEPT_LOG_LEVEL=$([[ "${{ env.ENVIRONMENT }}" == "prod" ]] && echo "INFO" || echo "DEBUG"),CONCEPT_UPSTASH_REDIS_PORT=6379" \
            --set-secrets="$SECRETS_ARG" \
            --source="./backend" \
            --allow-unauthenticated \
            --quiet

      - name: Deploy API to Compute Engine MIG (Rolling Update)
        run: |
          IMAGE_URL="${{ env.REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/${{ env.ARTIFACT_REGISTRY_REPO_NAME }}/concept-api-${{ env.ENVIRONMENT }}:${{ github.sha }}"
          # Use a shorter hash (first 8 characters) to avoid name length issues
          SHORT_SHA=$(echo "${{ github.sha }}" | cut -c1-8)
          TEMPLATE_NAME="${{ env.NAMING_PREFIX }}-api-tpl-${SHORT_SHA}"
          MIG_NAME="${{ env.NAMING_PREFIX }}-api-igm-${{ env.ENVIRONMENT }}"

          echo "Target Template Name: $TEMPLATE_NAME"
          echo "MIG Name: $MIG_NAME"
          echo "New Image: $IMAGE_URL"

          # Get the static IP that Terraform has created for this environment
          STATIC_IP_NAME="${{ env.NAMING_PREFIX }}-api-ip-${{ env.ENVIRONMENT }}"
          STATIC_IP=$(gcloud compute addresses describe "$STATIC_IP_NAME" \
            --project="${{ env.GCP_PROJECT_ID }}" \
            --region="${{ env.REGION }}" \
            --format="get(address)")

          echo "Using static IP: $STATIC_IP"

          # Check if the instance template already exists
          echo "Checking if instance template '$TEMPLATE_NAME' already exists..."
          if gcloud compute instance-templates describe "$TEMPLATE_NAME" --project="${{ env.GCP_PROJECT_ID }}" --region="${{ env.REGION }}" --quiet &> /dev/null; then
            echo "Instance template '$TEMPLATE_NAME' already exists. Skipping creation."
          else
            echo "Creating new instance template '$TEMPLATE_NAME'..."
            # Create a new instance template with the required configuration
            gcloud compute instance-templates create "$TEMPLATE_NAME" \
              --project="${{ env.GCP_PROJECT_ID }}" \
              --region="${{ env.REGION }}" \
              --machine-type=e2-micro \
              --network-tier=$([[ "${{ env.ENVIRONMENT }}" == "prod" ]] && echo "PREMIUM" || echo "STANDARD") \
              --maintenance-policy=MIGRATE \
              --provisioning-model=STANDARD \
              --service-account="${{ env.API_SERVICE_ACCOUNT_EMAIL }}" \
              --scopes=https://www.googleapis.com/auth/cloud-platform \
              --tags=http-server,https-server,concept-api,${{ env.NAMING_PREFIX }}-api-vm-${{ env.ENVIRONMENT }} \
              --create-disk=auto-delete=yes,boot=yes,device-name=boot,image-family=debian-11,image-project=debian-cloud,mode=rw,size=10,type=pd-balanced \
              --subnet=projects/${{ env.GCP_PROJECT_ID }}/regions/${{ env.REGION }}/subnetworks/default \
              --address="$STATIC_IP" \
              --metadata=startup-script-url=gs://${{ env.NAMING_PREFIX }}-assets-${{ env.ENVIRONMENT }}/startup-scripts/api-startup.sh,environment=${{ env.ENVIRONMENT }},naming_prefix=${{ env.NAMING_PREFIX }},region=${{ env.REGION }},docker-image="$IMAGE_URL" \
              --description="Template for Concept API instances (${{ github.sha }})"
            # Check the exit code of the create command
            if [ $? -ne 0 ]; then
              echo "Error: Failed to create instance template '$TEMPLATE_NAME'."
              exit 1
            fi
            echo "Instance template '$TEMPLATE_NAME' created successfully."
          fi

          # Update the Instance Group to use the new template
          echo "Setting new instance template on the Instance Group Manager"
          gcloud compute instance-groups managed set-instance-template "$MIG_NAME" \
            --template="$TEMPLATE_NAME" \
            --zone="${{ env.GCP_ZONE }}" \
            --project="${{ env.GCP_PROJECT_ID }}"

          # Get existing instances in the MIG
          echo "Getting existing instances in the group"
          INSTANCES=$(gcloud compute instance-groups managed list-instances "$MIG_NAME" \
            --zone="${{ env.GCP_ZONE }}" \
            --project="${{ env.GCP_PROJECT_ID }}" \
            --format="value(name)")

          # For each instance, recreate it with the new template
          if [ -n "$INSTANCES" ]; then
            for INSTANCE in $INSTANCES; do
              echo "Recreating instance $INSTANCE with new template"
              gcloud compute instance-groups managed recreate-instances "$MIG_NAME" \
                --instances="$INSTANCE" \
                --zone="${{ env.GCP_ZONE }}" \
                --project="${{ env.GCP_PROJECT_ID }}"
            done
          else
            echo "No instances found in the MIG, skipping recreation"
          fi

          # Wait for the instances to be stable
          echo "Waiting for instance group to become stable"
          gcloud compute instance-groups managed wait-until \
            --stable \
            --zone="${{ env.GCP_ZONE }}" \
            --project="${{ env.GCP_PROJECT_ID }}" \
            --timeout=300 \
            "$MIG_NAME"

      - name: Get Latest Vercel Deployment URL
        id: get_vercel_url
        if: env.ENVIRONMENT == 'dev' || env.ENVIRONMENT == 'prod'
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          PROJECT_ID_VERCEL: ${{ env.VERCEL_PROJECT_ID }}
          # Pass the commit SHA from the parent workflow that triggered this one
          COMMIT_SHA_FROM_PARENT_WORKFLOW: ${{ github.event.workflow_run.head_sha }}
          BRANCH_NAME_FROM_PARENT_WORKFLOW: ${{ github.event.workflow_run.head_branch }}
        run: |
          echo "Fetching deployment URL for Vercel Project ID: $PROJECT_ID_VERCEL"
          echo "Branch: $BRANCH_NAME_FROM_PARENT_WORKFLOW, Commit: $COMMIT_SHA_FROM_PARENT_WORKFLOW"

          SCOPE_ARG=""
          if [[ -n "$VERCEL_ORG_ID" ]]; then
            SCOPE_ARG="--scope $VERCEL_ORG_ID"
          fi

          # Instead of using vercel CLI commands with json output, use the Vercel REST API
          echo "Fetching deployments from Vercel API..."

          # Create Vercel config directory and credentials file
          mkdir -p ~/.vercel
          echo "{\"token\":\"$VERCEL_TOKEN\"}" > ~/.vercel/credentials.json

          # Use vercel CLI to get the project name
          PROJECT_NAME=$(vercel project ls $SCOPE_ARG | grep "$PROJECT_ID_VERCEL" | awk '{print $1}')
          echo "Project name: $PROJECT_NAME"

          if [[ -z "$PROJECT_NAME" ]]; then
            echo "Could not determine project name from project ID. Using direct API calls."

            # Use curl with Vercel API to get deployment information
            API_URL="https://api.vercel.com/v6/deployments"
            if [[ -n "$VERCEL_ORG_ID" ]]; then
              API_URL="${API_URL}?teamId=${VERCEL_ORG_ID}"
            fi

            DEPLOYMENTS_OUTPUT=$(curl -s -H "Authorization: Bearer $VERCEL_TOKEN" "$API_URL")
            LATEST_DEPLOYMENT_URL=""

            # If this is the main branch, look for production deployments
            if [[ "$BRANCH_NAME_FROM_PARENT_WORKFLOW" == "main" ]]; then
              echo "Looking for production deployments for main branch..."
              LATEST_DEPLOYMENT_URL=$(echo "$DEPLOYMENTS_OUTPUT" | jq -r '.deployments[] | select(.target == "production") | .url' | head -n 1)

              # If we found a production URL, it might be an alias
              if [[ -n "$LATEST_DEPLOYMENT_URL" && "$LATEST_DEPLOYMENT_URL" != "null" ]]; then
                # Check if this is an alias domain
                DOMAINS_OUTPUT=$(curl -s -H "Authorization: Bearer $VERCEL_TOKEN" "https://api.vercel.com/v9/projects/$PROJECT_ID_VERCEL/domains")
                PROD_DOMAIN=$(echo "$DOMAINS_OUTPUT" | jq -r '.domains[] | select(.apexName != null) | .name' | head -n 1)

                if [[ -n "$PROD_DOMAIN" && "$PROD_DOMAIN" != "null" ]]; then
                  LATEST_DEPLOYMENT_URL="https://$PROD_DOMAIN"
                  echo "Found production domain: $LATEST_DEPLOYMENT_URL"
                else
                  LATEST_DEPLOYMENT_URL="https://$LATEST_DEPLOYMENT_URL"
                  echo "Using deployment URL: $LATEST_DEPLOYMENT_URL"
                fi
              fi
            fi

            # If we still don't have a URL, try to get the most recent deployment
            if [[ -z "$LATEST_DEPLOYMENT_URL" || "$LATEST_DEPLOYMENT_URL" == "null" ]]; then
              echo "Falling back to most recent deployment..."
              LATEST_DEPLOYMENT_URL=$(echo "$DEPLOYMENTS_OUTPUT" | jq -r '.deployments[0].url' | head -n 1)

              if [[ -n "$LATEST_DEPLOYMENT_URL" && "$LATEST_DEPLOYMENT_URL" != "null" ]]; then
                LATEST_DEPLOYMENT_URL="https://$LATEST_DEPLOYMENT_URL"
              fi
            fi

            DEPLOY_URL="$LATEST_DEPLOYMENT_URL"
          else
            # Use the vercel inspect command on the project name to get the URL
            echo "Using vercel inspect to get deployment URL..."

            # For production/main branch
            if [[ "$BRANCH_NAME_FROM_PARENT_WORKFLOW" == "main" ]]; then
              # Try to get info about the current production deployment
              INSPECT_OUTPUT=$(vercel inspect "$PROJECT_NAME" $SCOPE_ARG)

              # Extract URL from the output using grep and awk
              DEPLOY_URL=$(echo "$INSPECT_OUTPUT" | grep -A 1 "Production URL" | tail -n 1 | awk '{print $NF}')

              # If no production URL is found, try getting the latest deployment
              if [[ -z "$DEPLOY_URL" || "$DEPLOY_URL" == "null" ]]; then
                echo "No production URL found, getting latest deployment..."
                LATEST_OUTPUT=$(vercel ls "$PROJECT_NAME" $SCOPE_ARG | grep -v 'Production\|Name\|To deploy\|Deployments' | head -n 1)
                LATEST_ID=$(echo "$LATEST_OUTPUT" | awk '{print $1}')

                if [[ -n "$LATEST_ID" && "$LATEST_ID" != "null" ]]; then
                  DEPLOY_INSPECT=$(vercel inspect "$LATEST_ID" $SCOPE_ARG)
                  DEPLOY_URL=$(echo "$DEPLOY_INSPECT" | grep "URL:" | head -n 1 | awk '{print $NF}')
                  DEPLOY_URL="https://$DEPLOY_URL"
                fi
              fi
            else
              # For non-production branches, get the most recent deployment
              LATEST_OUTPUT=$(vercel ls "$PROJECT_NAME" $SCOPE_ARG | grep -v 'Production\|Name\|To deploy\|Deployments' | head -n 1)
              LATEST_ID=$(echo "$LATEST_OUTPUT" | awk '{print $1}')

              if [[ -n "$LATEST_ID" && "$LATEST_ID" != "null" ]]; then
                DEPLOY_INSPECT=$(vercel inspect "$LATEST_ID" $SCOPE_ARG)
                DEPLOY_URL=$(echo "$DEPLOY_INSPECT" | grep "URL:" | head -n 1 | awk '{print $NF}')
                DEPLOY_URL="https://$DEPLOY_URL"
              fi
            fi
          fi

          # Validate the URL
          if [[ -z "$DEPLOY_URL" || "$DEPLOY_URL" == "null" ]]; then
            echo "Error: Could not determine a valid deployment URL."

            # Final fallback - hardcode the URL based on project name pattern
            if [[ "$BRANCH_NAME_FROM_PARENT_WORKFLOW" == "main" && -n "$PROJECT_NAME" ]]; then
              DEPLOY_URL="https://$PROJECT_NAME.vercel.app"
              echo "Using fallback URL based on project name: $DEPLOY_URL"
            else
              exit 1
            fi
          fi

          echo "Final Vercel Deployment URL for Monitoring: $DEPLOY_URL"
          echo "VERCEL_DEPLOYMENT_URL_FOR_MONITORING=$DEPLOY_URL" >> $GITHUB_ENV
          echo "::set-output name=url::$DEPLOY_URL"

      - name: Update GCP Frontend Uptime Check
        if: env.FRONTEND_UPTIME_CHECK_ID != '' && env.VERCEL_DEPLOYMENT_URL_FOR_MONITORING != ''
        env:
          GCP_PROJECT_ID_FOR_MONITOR: ${{ env.GCP_PROJECT_ID }}
          UPTIME_CHECK_ID_FOR_MONITOR: ${{ env.FRONTEND_UPTIME_CHECK_ID }}
          VERCEL_URL_TO_MONITOR: ${{ env.VERCEL_DEPLOYMENT_URL_FOR_MONITORING }}
        run: |
          echo "Updating Frontend Uptime Check for environment: ${{ env.ENVIRONMENT }}"
          echo "Monitoring URL: $VERCEL_URL_TO_MONITOR"

          # Extract hostname from URL, handling both http:// and https:// prefixes
          HOSTNAME=$(echo "$VERCEL_URL_TO_MONITOR" | sed -E 's~^https?://~~' | cut -d/ -f1)
          echo "Extracted hostname: $HOSTNAME"

          # Install Terraform
          echo "Installing Terraform..."
          TF_VERSION="1.5.7"
          wget -q -O terraform.zip https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip
          unzip -q terraform.zip
          chmod +x terraform
          mkdir -p /tmp/bin
          mv terraform /tmp/bin/
          export PATH="/tmp/bin:$PATH"
          terraform --version

          # Create directories for temporary files
          mkdir -p /tmp/terraform_monitor
          mkdir -p /tmp/monitor

          # Create a minimal Terraform file to update the uptime check
          cat > /tmp/terraform_monitor/main.tf << EOF
          provider "google" {
            project = "$GCP_PROJECT_ID_FOR_MONITOR"
            region  = "${{ env.REGION }}"
          }

          # Import the existing uptime check resource
          resource "google_monitoring_uptime_check_config" "frontend_uptime_check" {
            display_name = "${{ env.NAMING_PREFIX }}-frontend-uptime-${{ env.ENVIRONMENT }}"
            timeout      = "10s"
            period       = "60s"

            http_check {
              path         = "/"
              port         = "443"
              use_ssl      = true
              validate_ssl = true
              host         = "$HOSTNAME"
            }

            monitored_resource {
              type = "uptime_url"
              labels = {
                project_id = "$GCP_PROJECT_ID_FOR_MONITOR"
                host       = "$HOSTNAME"
              }
            }

            lifecycle {
              create_before_destroy = true
            }
          }
          EOF

          # Initialize Terraform
          cd /tmp/terraform_monitor
          terraform init

          # Import existing uptime check resource if it exists
          terraform import google_monitoring_uptime_check_config.frontend_uptime_check \
            projects/$GCP_PROJECT_ID_FOR_MONITOR/uptimeCheckConfigs/$UPTIME_CHECK_ID_FOR_MONITOR || \
            echo "Resource could not be imported, continuing with apply"

          # Apply Terraform changes
          terraform apply -auto-approve

          # Check if Terraform apply was successful
          if [ $? -eq 0 ]; then
            echo "Successfully updated uptime check to monitor: $HOSTNAME using Terraform"

            # Get the new uptime check ID from Terraform state
            NEW_CHECK_ID=$(terraform state show google_monitoring_uptime_check_config.frontend_uptime_check | grep "^id" | cut -d= -f2 | tr -d ' "')
            echo "New uptime check ID: $NEW_CHECK_ID"

            # List all uptime checks with the same display name
            echo "Checking for any orphaned uptime checks..."
            DISPLAY_NAME="${{ env.NAMING_PREFIX }}-frontend-uptime-${{ env.ENVIRONMENT }}"
            ORPHANED_CHECKS=$(gcloud monitoring uptime-check-configs list \
              --project=$GCP_PROJECT_ID_FOR_MONITOR \
              --filter="displayName:\"$DISPLAY_NAME\" AND NOT id:\"$NEW_CHECK_ID\"" \
              --format="value(name)")

            # Delete any orphaned checks that might exist
            if [ -n "$ORPHANED_CHECKS" ]; then
              echo "Found orphaned uptime checks, cleaning up..."
              for CHECK in $ORPHANED_CHECKS; do
                CHECK_ID=$(echo $CHECK | sed 's|.*/||')
                echo "Deleting orphaned check: $CHECK_ID"
                gcloud monitoring uptime-check-configs delete $CHECK_ID --project=$GCP_PROJECT_ID_FOR_MONITOR --quiet
              done
            else
              echo "No orphaned uptime checks found."
            fi
          else
            echo "Warning: Terraform approach failed. Falling back to direct gcloud update..."

            # Fallback approach: Create new uptime check with gcloud
            DISPLAY_NAME="${{ env.NAMING_PREFIX }}-frontend-uptime-${{ env.ENVIRONMENT }}"
            NEW_CHECK_ID="${{ env.NAMING_PREFIX }}-frontend-check-${{ env.ENVIRONMENT }}-$(date +%s)"

            echo "Creating new uptime check with ID: $NEW_CHECK_ID"
            gcloud monitoring uptime-check-configs create $NEW_CHECK_ID \
              --display-name="$DISPLAY_NAME" \
              --http-check-path="/" \
              --http-check-port=443 \
              --http-check-use-ssl \
              --http-check-validate-ssl \
              --http-check-host="$HOSTNAME" \
              --period=60s \
              --timeout=10s \
              --project=$GCP_PROJECT_ID_FOR_MONITOR

            if [ $? -eq 0 ]; then
              echo "Successfully created new uptime check via gcloud"

              # Now delete the old check if different
              if [ "$UPTIME_CHECK_ID_FOR_MONITOR" != "$NEW_CHECK_ID" ]; then
                echo "Deleting old uptime check: $UPTIME_CHECK_ID_FOR_MONITOR"
                gcloud monitoring uptime-check-configs delete $UPTIME_CHECK_ID_FOR_MONITOR \
                  --project=$GCP_PROJECT_ID_FOR_MONITOR --quiet || \
                  echo "Warning: Could not delete old uptime check, it may have been deleted already"
              fi

              # We need to update the GitHub secret for next time
              echo "IMPORTANT: Update your GitHub secret with the new uptime check ID: $NEW_CHECK_ID"
            else
              echo "Warning: Failed to create new uptime check via gcloud"
            fi
          fi
