name: Deploy Backend and Update Frontend Monitor

on:
  workflow_run:
    workflows: ["CI Tests & Deployment"]
    types:
      - completed
    branches: [develop, main]

jobs:
  deploy:
    name: Deploy to GCP
    # Only run if the CI workflow succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha || github.ref }}

      # Set environment-specific variables
      - name: Set Environment Specifics
        id: set_env
        run: |
          if [[ "${{ github.event.workflow_run.head_branch || github.ref_name }}" == "develop" ]]; then
            echo "ENVIRONMENT=dev" >> $GITHUB_ENV
            echo "GCP_PROJECT_ID=${{ secrets.DEV_GCP_PROJECT_ID }}" >> $GITHUB_ENV
            echo "GCP_ZONE=${{ secrets.DEV_GCP_ZONE }}" >> $GITHUB_ENV
            echo "NAMING_PREFIX=${{ secrets.DEV_NAMING_PREFIX }}" >> $GITHUB_ENV
            echo "WORKLOAD_IDENTITY_PROVIDER=${{ secrets.DEV_WORKLOAD_IDENTITY_PROVIDER }}" >> $GITHUB_ENV
            echo "CICD_SERVICE_ACCOUNT=${{ secrets.DEV_CICD_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "ARTIFACT_REGISTRY_REPO_NAME=${{ secrets.DEV_ARTIFACT_REGISTRY_REPO_NAME }}" >> $GITHUB_ENV
            echo "WORKER_SERVICE_ACCOUNT_EMAIL=${{ secrets.DEV_WORKER_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "API_SERVICE_ACCOUNT_EMAIL=${{ secrets.DEV_API_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "WORKER_MIN_INSTANCES=${{ secrets.DEV_WORKER_MIN_INSTANCES || 0 }}" >> $GITHUB_ENV
            echo "WORKER_MAX_INSTANCES=${{ secrets.DEV_WORKER_MAX_INSTANCES || 5 }}" >> $GITHUB_ENV
            echo "FRONTEND_UPTIME_CHECK_ID=${{ secrets.DEV_FRONTEND_UPTIME_CHECK_CONFIG_ID }}" >> $GITHUB_ENV
            echo "VERCEL_PROJECT_ID=${{ secrets.DEV_VERCEL_PROJECT_ID }}" >> $GITHUB_ENV
            echo "FRONTEND_ALERT_POLICY_ID=${{ secrets.DEV_FRONTEND_ALERT_POLICY_ID }}" >> $GITHUB_ENV
            echo "ALERT_NOTIFICATION_CHANNEL_FULL_ID=${{ secrets.DEV_ALERT_NOTIFICATION_CHANNEL_FULL_ID }}" >> $GITHUB_ENV
            echo "FRONTEND_STARTUP_ALERT_DELAY=${{ secrets.DEV_FRONTEND_STARTUP_ALERT_DELAY || '2100s' }}" >> $GITHUB_ENV
            echo "ALERT_ALIGNMENT_PERIOD=${{ secrets.DEV_ALERT_ALIGNMENT_PERIOD || '300s' }}" >> $GITHUB_ENV
          elif [[ "${{ github.event.workflow_run.head_branch || github.ref_name }}" == "main" ]]; then
            echo "ENVIRONMENT=prod" >> $GITHUB_ENV
            echo "GCP_PROJECT_ID=${{ secrets.PROD_GCP_PROJECT_ID }}" >> $GITHUB_ENV
            echo "GCP_ZONE=${{ secrets.PROD_GCP_ZONE }}" >> $GITHUB_ENV
            echo "NAMING_PREFIX=${{ secrets.PROD_NAMING_PREFIX }}" >> $GITHUB_ENV
            echo "WORKLOAD_IDENTITY_PROVIDER=${{ secrets.PROD_WORKLOAD_IDENTITY_PROVIDER }}" >> $GITHUB_ENV
            echo "CICD_SERVICE_ACCOUNT=${{ secrets.PROD_CICD_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "ARTIFACT_REGISTRY_REPO_NAME=${{ secrets.PROD_ARTIFACT_REGISTRY_REPO_NAME }}" >> $GITHUB_ENV
            echo "WORKER_SERVICE_ACCOUNT_EMAIL=${{ secrets.PROD_WORKER_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "API_SERVICE_ACCOUNT_EMAIL=${{ secrets.PROD_API_SERVICE_ACCOUNT_EMAIL }}" >> $GITHUB_ENV
            echo "WORKER_MIN_INSTANCES=${{ secrets.PROD_WORKER_MIN_INSTANCES || 0 }}" >> $GITHUB_ENV
            echo "WORKER_MAX_INSTANCES=${{ secrets.PROD_WORKER_MAX_INSTANCES || 5 }}" >> $GITHUB_ENV
            echo "FRONTEND_UPTIME_CHECK_ID=${{ secrets.PROD_FRONTEND_UPTIME_CHECK_CONFIG_ID }}" >> $GITHUB_ENV
            echo "VERCEL_PROJECT_ID=${{ secrets.PROD_VERCEL_PROJECT_ID }}" >> $GITHUB_ENV
            echo "FRONTEND_ALERT_POLICY_ID=${{ secrets.PROD_FRONTEND_ALERT_POLICY_ID }}" >> $GITHUB_ENV
            echo "ALERT_NOTIFICATION_CHANNEL_FULL_ID=${{ secrets.PROD_ALERT_NOTIFICATION_CHANNEL_FULL_ID }}" >> $GITHUB_ENV
            echo "FRONTEND_STARTUP_ALERT_DELAY=${{ secrets.PROD_FRONTEND_STARTUP_ALERT_DELAY || '2100s' }}" >> $GITHUB_ENV
            echo "ALERT_ALIGNMENT_PERIOD=${{ secrets.PROD_ALERT_ALIGNMENT_PERIOD || '300s' }}" >> $GITHUB_ENV
          else
            echo "Branch is not develop or main, skipping deployment."
            exit 0
          fi
          echo "REGION=${{ secrets.GCP_REGION }}" >> $GITHUB_ENV

      # Authenticate with the appropriate environment credentials
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ env.CICD_SERVICE_ACCOUNT }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.7

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18

      - name: Install dependencies
        run: |
          npm install -g vercel
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Deploy Cloud Function Worker
        run: |
          FUNCTION_NAME="${{ env.NAMING_PREFIX }}-worker-${{ env.ENVIRONMENT }}"
          TASKS_TOPIC_NAME="${{ env.NAMING_PREFIX }}-tasks-${{ env.ENVIRONMENT }}"

          echo "Deploying Function: $FUNCTION_NAME"
          echo "Trigger Topic: $TASKS_TOPIC_NAME"
          echo "Service Account: ${{ env.WORKER_SERVICE_ACCOUNT_EMAIL }}"

          # Construct --set-secrets argument dynamically
          SECRETS_ARG=""
          SECRETS_ARG+="CONCEPT_SUPABASE_URL=${{ env.NAMING_PREFIX }}-supabase-url-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_SUPABASE_KEY=${{ env.NAMING_PREFIX }}-supabase-key-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_SUPABASE_SERVICE_ROLE=${{ env.NAMING_PREFIX }}-supabase-service-role-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_SUPABASE_JWT_SECRET=${{ env.NAMING_PREFIX }}-supabase-jwt-secret-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_JIGSAWSTACK_API_KEY=${{ env.NAMING_PREFIX }}-jigsawstack-api-key-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_UPSTASH_REDIS_ENDPOINT=${{ env.NAMING_PREFIX }}-redis-endpoint-${{ env.ENVIRONMENT }}:latest,"
          SECRETS_ARG+="CONCEPT_UPSTASH_REDIS_PASSWORD=${{ env.NAMING_PREFIX }}-redis-password-${{ env.ENVIRONMENT }}:latest"

          # Deploy the function using the 'backend' directory as source with the shim main.py
          gcloud functions deploy "$FUNCTION_NAME" \
            --gen2 \
            --region="${{ env.REGION }}" \
            --project="${{ env.GCP_PROJECT_ID }}" \
            --runtime=python311 \
            --entry-point=handle_pubsub \
            --run-service-account="${{ env.WORKER_SERVICE_ACCOUNT_EMAIL }}" \
            --trigger-topic="$TASKS_TOPIC_NAME" \
            --timeout=540s \
            --memory=2048Mi \
            --min-instances=${{ env.WORKER_MIN_INSTANCES }} \
            --max-instances=${{ env.WORKER_MAX_INSTANCES }} \
            --set-env-vars="ENVIRONMENT=${{ env.ENVIRONMENT }},GCP_PROJECT_ID=${{ env.GCP_PROJECT_ID }},CONCEPT_STORAGE_BUCKET_CONCEPT=concept-images-${{ env.ENVIRONMENT }},CONCEPT_STORAGE_BUCKET_PALETTE=palette-images-${{ env.ENVIRONMENT }},CONCEPT_DB_TABLE_TASKS=tasks_${{ env.ENVIRONMENT }},CONCEPT_DB_TABLE_CONCEPTS=concepts_${{ env.ENVIRONMENT }},CONCEPT_DB_TABLE_PALETTES=color_variations_${{ env.ENVIRONMENT }},CONCEPT_LOG_LEVEL=$([[ "${{ env.ENVIRONMENT }}" == "prod" ]] && echo "INFO" || echo "DEBUG"),CONCEPT_UPSTASH_REDIS_PORT=6379" \
            --set-secrets="$SECRETS_ARG" \
            --source="./backend" \
            --allow-unauthenticated \
            --quiet

      - name: Deploy API to Compute Engine MIG (Rolling Update)
        run: |
          IMAGE_URL="${{ env.REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/${{ env.ARTIFACT_REGISTRY_REPO_NAME }}/concept-api-${{ env.ENVIRONMENT }}:${{ github.sha }}"
          # Use a shorter hash (first 8 characters) to avoid name length issues
          SHORT_SHA=$(echo "${{ github.sha }}" | cut -c1-8)
          TEMPLATE_NAME="${{ env.NAMING_PREFIX }}-api-tpl-${SHORT_SHA}"
          MIG_NAME="${{ env.NAMING_PREFIX }}-api-igm-${{ env.ENVIRONMENT }}"

          echo "Target Template Name: $TEMPLATE_NAME"
          echo "MIG Name: $MIG_NAME"
          echo "New Image: $IMAGE_URL"

          # Get the static IP that Terraform has created for this environment
          STATIC_IP_NAME="${{ env.NAMING_PREFIX }}-api-ip-${{ env.ENVIRONMENT }}"
          STATIC_IP=$(gcloud compute addresses describe "$STATIC_IP_NAME" \
            --project="${{ env.GCP_PROJECT_ID }}" \
            --region="${{ env.REGION }}" \
            --format="get(address)")

          echo "Using static IP: $STATIC_IP"

          # Check if the instance template already exists
          echo "Checking if instance template '$TEMPLATE_NAME' already exists..."
          if gcloud compute instance-templates describe "$TEMPLATE_NAME" --project="${{ env.GCP_PROJECT_ID }}" --region="${{ env.REGION }}" --quiet &> /dev/null; then
            echo "Instance template '$TEMPLATE_NAME' already exists. Skipping creation."
          else
            echo "Creating new instance template '$TEMPLATE_NAME'..."
            # Create a new instance template with the required configuration
            gcloud compute instance-templates create "$TEMPLATE_NAME" \
              --project="${{ env.GCP_PROJECT_ID }}" \
              --region="${{ env.REGION }}" \
              --machine-type=e2-micro \
              --network-tier=$([[ "${{ env.ENVIRONMENT }}" == "prod" ]] && echo "PREMIUM" || echo "STANDARD") \
              --maintenance-policy=MIGRATE \
              --provisioning-model=STANDARD \
              --service-account="${{ env.API_SERVICE_ACCOUNT_EMAIL }}" \
              --scopes=https://www.googleapis.com/auth/cloud-platform \
              --tags=http-server,https-server,concept-api,${{ env.NAMING_PREFIX }}-api-vm-${{ env.ENVIRONMENT }} \
              --create-disk=auto-delete=yes,boot=yes,device-name=boot,image-family=debian-11,image-project=debian-cloud,mode=rw,size=10,type=pd-balanced \
              --subnet=projects/${{ env.GCP_PROJECT_ID }}/regions/${{ env.REGION }}/subnetworks/default \
              --address="$STATIC_IP" \
              --metadata=startup-script-url=gs://${{ env.NAMING_PREFIX }}-assets-${{ env.ENVIRONMENT }}/startup-scripts/api-startup.sh,environment=${{ env.ENVIRONMENT }},naming_prefix=${{ env.NAMING_PREFIX }},region=${{ env.REGION }},docker-image="$IMAGE_URL" \
              --description="Template for Concept API instances (${{ github.sha }})"
            # Check the exit code of the create command
            if [ $? -ne 0 ]; then
              echo "Error: Failed to create instance template '$TEMPLATE_NAME'."
              exit 1
            fi
            echo "Instance template '$TEMPLATE_NAME' created successfully."
          fi

          # Update the Instance Group to use the new template
          echo "Setting new instance template on the Instance Group Manager"
          gcloud compute instance-groups managed set-instance-template "$MIG_NAME" \
            --template="$TEMPLATE_NAME" \
            --zone="${{ env.GCP_ZONE }}" \
            --project="${{ env.GCP_PROJECT_ID }}"

          # Get existing instances in the MIG
          echo "Getting existing instances in the group"
          INSTANCES=$(gcloud compute instance-groups managed list-instances "$MIG_NAME" \
            --zone="${{ env.GCP_ZONE }}" \
            --project="${{ env.GCP_PROJECT_ID }}" \
            --format="value(name)")

          # For each instance, recreate it with the new template
          if [ -n "$INSTANCES" ]; then
            for INSTANCE in $INSTANCES; do
              echo "Recreating instance $INSTANCE with new template"
              gcloud compute instance-groups managed recreate-instances "$MIG_NAME" \
                --instances="$INSTANCE" \
                --zone="${{ env.GCP_ZONE }}" \
                --project="${{ env.GCP_PROJECT_ID }}"
            done
          else
            echo "No instances found in the MIG, skipping recreation"
          fi

          # Wait for the instances to be stable
          echo "Waiting for instance group to become stable"
          gcloud compute instance-groups managed wait-until \
            --stable \
            --zone="${{ env.GCP_ZONE }}" \
            --project="${{ env.GCP_PROJECT_ID }}" \
            --timeout=300 \
            "$MIG_NAME"

      - name: Get Latest Vercel Deployment URL
        id: get_vercel_url
        if: env.ENVIRONMENT == 'dev' || env.ENVIRONMENT == 'prod'
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          PROJECT_ID_VERCEL: ${{ env.VERCEL_PROJECT_ID }}
          # Pass the commit SHA from the parent workflow that triggered this one
          COMMIT_SHA_FROM_PARENT_WORKFLOW: ${{ github.event.workflow_run.head_sha }}
          BRANCH_NAME_FROM_PARENT_WORKFLOW: ${{ github.event.workflow_run.head_branch }}
        run: |
          echo "Fetching deployment URL for Vercel Project ID: $PROJECT_ID_VERCEL"
          echo "Branch: $BRANCH_NAME_FROM_PARENT_WORKFLOW, Commit: $COMMIT_SHA_FROM_PARENT_WORKFLOW"

          SCOPE_ARG=""
          if [[ -n "$VERCEL_ORG_ID" ]]; then
            SCOPE_ARG="--scope $VERCEL_ORG_ID"
          fi

          # Instead of using vercel CLI commands with json output, use the Vercel REST API
          echo "Fetching deployments from Vercel API..."

          # Create Vercel config directory and credentials file
          mkdir -p ~/.vercel
          echo "{\"token\":\"$VERCEL_TOKEN\"}" > ~/.vercel/credentials.json

          # Use vercel CLI to get the project name
          PROJECT_NAME=$(vercel project ls $SCOPE_ARG | grep "$PROJECT_ID_VERCEL" | awk '{print $1}')
          echo "Project name: $PROJECT_NAME"

          if [[ -z "$PROJECT_NAME" ]]; then
            echo "Could not determine project name from project ID. Using direct API calls."

            # Use curl with Vercel API to get deployment information
            API_URL="https://api.vercel.com/v6/deployments"
            if [[ -n "$VERCEL_ORG_ID" ]]; then
              API_URL="${API_URL}?teamId=${VERCEL_ORG_ID}"
            fi

            DEPLOYMENTS_OUTPUT=$(curl -s -H "Authorization: Bearer $VERCEL_TOKEN" "$API_URL")
            LATEST_DEPLOYMENT_URL=""

            # If this is the main branch, look for production deployments
            if [[ "$BRANCH_NAME_FROM_PARENT_WORKFLOW" == "main" ]]; then
              echo "Looking for production deployments for main branch..."
              LATEST_DEPLOYMENT_URL=$(echo "$DEPLOYMENTS_OUTPUT" | jq -r '.deployments[] | select(.target == "production") | .url' | head -n 1)

              # If we found a production URL, it might be an alias
              if [[ -n "$LATEST_DEPLOYMENT_URL" && "$LATEST_DEPLOYMENT_URL" != "null" ]]; then
                # Check if this is an alias domain
                DOMAINS_OUTPUT=$(curl -s -H "Authorization: Bearer $VERCEL_TOKEN" "https://api.vercel.com/v9/projects/$PROJECT_ID_VERCEL/domains")
                PROD_DOMAIN=$(echo "$DOMAINS_OUTPUT" | jq -r '.domains[] | select(.apexName != null) | .name' | head -n 1)

                if [[ -n "$PROD_DOMAIN" && "$PROD_DOMAIN" != "null" ]]; then
                  LATEST_DEPLOYMENT_URL="https://$PROD_DOMAIN"
                  echo "Found production domain: $LATEST_DEPLOYMENT_URL"
                else
                  LATEST_DEPLOYMENT_URL="https://$LATEST_DEPLOYMENT_URL"
                  echo "Using deployment URL: $LATEST_DEPLOYMENT_URL"
                fi
              fi
            fi

            # If we still don't have a URL, try to get the most recent deployment
            if [[ -z "$LATEST_DEPLOYMENT_URL" || "$LATEST_DEPLOYMENT_URL" == "null" ]]; then
              echo "Falling back to most recent deployment..."
              LATEST_DEPLOYMENT_URL=$(echo "$DEPLOYMENTS_OUTPUT" | jq -r '.deployments[0].url' | head -n 1)

              if [[ -n "$LATEST_DEPLOYMENT_URL" && "$LATEST_DEPLOYMENT_URL" != "null" ]]; then
                LATEST_DEPLOYMENT_URL="https://$LATEST_DEPLOYMENT_URL"
              fi
            fi

            DEPLOY_URL="$LATEST_DEPLOYMENT_URL"
          else
            # Use the vercel inspect command on the project name to get the URL
            echo "Using vercel inspect to get deployment URL..."

            # For production/main branch
            if [[ "$BRANCH_NAME_FROM_PARENT_WORKFLOW" == "main" ]]; then
              # Try to get info about the current production deployment
              INSPECT_OUTPUT=$(vercel inspect "$PROJECT_NAME" $SCOPE_ARG)

              # Extract URL from the output using grep and awk
              DEPLOY_URL=$(echo "$INSPECT_OUTPUT" | grep -A 1 "Production URL" | tail -n 1 | awk '{print $NF}')

              # If no production URL is found, try getting the latest deployment
              if [[ -z "$DEPLOY_URL" || "$DEPLOY_URL" == "null" ]]; then
                echo "No production URL found, getting latest deployment..."
                LATEST_OUTPUT=$(vercel ls "$PROJECT_NAME" $SCOPE_ARG | grep -v 'Production\|Name\|To deploy\|Deployments' | head -n 1)
                LATEST_ID=$(echo "$LATEST_OUTPUT" | awk '{print $1}')

                if [[ -n "$LATEST_ID" && "$LATEST_ID" != "null" ]]; then
                  DEPLOY_INSPECT=$(vercel inspect "$LATEST_ID" $SCOPE_ARG)
                  DEPLOY_URL=$(echo "$DEPLOY_INSPECT" | grep "URL:" | head -n 1 | awk '{print $NF}')
                  DEPLOY_URL="https://$DEPLOY_URL"
                fi
              fi
            else
              # For non-production branches, get the most recent deployment
              LATEST_OUTPUT=$(vercel ls "$PROJECT_NAME" $SCOPE_ARG | grep -v 'Production\|Name\|To deploy\|Deployments' | head -n 1)
              LATEST_ID=$(echo "$LATEST_OUTPUT" | awk '{print $1}')

              if [[ -n "$LATEST_ID" && "$LATEST_ID" != "null" ]]; then
                DEPLOY_INSPECT=$(vercel inspect "$LATEST_ID" $SCOPE_ARG)
                DEPLOY_URL=$(echo "$DEPLOY_INSPECT" | grep "URL:" | head -n 1 | awk '{print $NF}')
                DEPLOY_URL="https://$DEPLOY_URL"
              fi
            fi
          fi

          # Validate the URL
          if [[ -z "$DEPLOY_URL" || "$DEPLOY_URL" == "null" ]]; then
            echo "Error: Could not determine a valid deployment URL."

            # Final fallback - hardcode the URL based on project name pattern
            if [[ "$BRANCH_NAME_FROM_PARENT_WORKFLOW" == "main" && -n "$PROJECT_NAME" ]]; then
              DEPLOY_URL="https://$PROJECT_NAME.vercel.app"
              echo "Using fallback URL based on project name: $DEPLOY_URL"
            else
              exit 1
            fi
          fi

          echo "Final Vercel Deployment URL for Monitoring: $DEPLOY_URL"
          echo "VERCEL_DEPLOYMENT_URL_FOR_MONITORING=$DEPLOY_URL" >> $GITHUB_ENV
          echo "::set-output name=url::$DEPLOY_URL"

      - name: Update GCP Frontend Uptime Check
        if: env.FRONTEND_UPTIME_CHECK_ID != '' && env.VERCEL_DEPLOYMENT_URL_FOR_MONITORING != '' && env.FRONTEND_ALERT_POLICY_ID != '' && env.ALERT_NOTIFICATION_CHANNEL_FULL_ID != ''
        env:
          GCP_PROJECT_ID_FOR_MONITOR: ${{ env.GCP_PROJECT_ID }}
          UPTIME_CHECK_ID_FOR_MONITOR: ${{ env.FRONTEND_UPTIME_CHECK_ID }}
          VERCEL_URL_TO_MONITOR: ${{ env.VERCEL_DEPLOYMENT_URL_FOR_MONITORING }}
          FRONTEND_ALERT_POLICY_ID: ${{ env.FRONTEND_ALERT_POLICY_ID }}
          ALERT_NOTIFICATION_CHANNEL_FULL_ID: ${{ env.ALERT_NOTIFICATION_CHANNEL_FULL_ID }}
          FRONTEND_STARTUP_ALERT_DELAY: ${{ env.FRONTEND_STARTUP_ALERT_DELAY || '300s' }}
          ALERT_ALIGNMENT_PERIOD: ${{ env.ALERT_ALIGNMENT_PERIOD || '300s' }}
          TF_STATE_BUCKET_NAME: ${{ secrets.TF_STATE_BUCKET_NAME }}
        run: |
          echo "Updating Frontend Uptime Check for environment: ${{ env.ENVIRONMENT }}"
          echo "Monitoring URL: $VERCEL_URL_TO_MONITOR"
          echo "Uptime Check ID (config): $UPTIME_CHECK_ID_FOR_MONITOR"
          echo "Alert Policy ID: $FRONTEND_ALERT_POLICY_ID"
          echo "Notification Channel ID: $ALERT_NOTIFICATION_CHANNEL_FULL_ID"
          echo "TF State Bucket: $TF_STATE_BUCKET_NAME"

          HOSTNAME=$(echo "$VERCEL_URL_TO_MONITOR" | sed -E 's#^https?://##' | cut -d/ -f1)
          echo "Extracted hostname: $HOSTNAME"

          # --- BEGIN: Clean Up Stale Dynamic Monitoring Resources (Hygiene) ---
          echo "Cleaning up potentially stale monitoring resources..."
          UPTIME_CHECK_DISPLAY_NAME_TO_CLEAN="${{ env.NAMING_PREFIX }}-frontend-availability-${{ env.ENVIRONMENT }}"
          ALERT_POLICY_DISPLAY_NAME_TO_CLEAN="${{ env.NAMING_PREFIX }}-frontend-down-al-${{ env.ENVIRONMENT }}"

          # Clean stale uptime checks
          STALE_UPTIME_CHECKS=$(gcloud monitoring uptime-check-configs list --project="${GCP_PROJECT_ID_FOR_MONITOR}" --filter="displayName=${UPTIME_CHECK_DISPLAY_NAME_TO_CLEAN}" --format="value(name)")
          KNOWN_UPTIME_CHECK_FULL_PATH="projects/${GCP_PROJECT_ID_FOR_MONITOR}/uptimeCheckConfigs/${UPTIME_CHECK_ID_FOR_MONITOR}"
          for STALE_CHECK_PATH in $STALE_UPTIME_CHECKS; do
            if [[ "$STALE_CHECK_PATH" != "$KNOWN_UPTIME_CHECK_FULL_PATH" ]]; then
              echo "Deleting stale uptime check: $STALE_CHECK_PATH"
              gcloud monitoring uptime-check-configs delete "$STALE_CHECK_PATH" --project="${GCP_PROJECT_ID_FOR_MONITOR}" --quiet || echo "Warning: Failed to delete stale uptime check $STALE_CHECK_PATH"
            fi
          done

          # Clean stale alert policies
          KNOWN_ALERT_POLICY_FULL_PATH="projects/${GCP_PROJECT_ID_FOR_MONITOR}/alertPolicies/${FRONTEND_ALERT_POLICY_ID}"
          if [[ ! "$FRONTEND_ALERT_POLICY_ID" == projects/* ]]; then # if short ID was passed
            KNOWN_ALERT_POLICY_FULL_PATH="projects/${GCP_PROJECT_ID_FOR_MONITOR}/alertPolicies/${FRONTEND_ALERT_POLICY_ID}"
          fi
          STALE_ALERT_POLICIES=$(gcloud alpha monitoring policies list --project="${GCP_PROJECT_ID_FOR_MONITOR}" --filter="displayName=${ALERT_POLICY_DISPLAY_NAME_TO_CLEAN}" --format="value(name)")
          for STALE_POLICY_PATH in $STALE_ALERT_POLICIES; do
            if [[ "$STALE_POLICY_PATH" != "$KNOWN_ALERT_POLICY_FULL_PATH" ]]; then
              echo "Deleting stale alert policy: $STALE_POLICY_PATH"
              gcloud alpha monitoring policies delete "$STALE_POLICY_PATH" --project="${GCP_PROJECT_ID_FOR_MONITOR}" --quiet || echo "Warning: Failed to delete stale alert policy $STALE_POLICY_PATH"
            fi
          done
          echo "Cleanup of stale monitoring resources attempt complete."
          # --- END: Clean Up Stale Dynamic Monitoring Resources ---

          mkdir -p /tmp/terraform_monitor
          cd /tmp/terraform_monitor

          # Construct full alert policy path if only short ID is given
          FULL_ALERT_POLICY_ID=$FRONTEND_ALERT_POLICY_ID
          if [[ ! "$FULL_ALERT_POLICY_ID" == projects/* ]]; then
            FULL_ALERT_POLICY_ID="projects/$GCP_PROJECT_ID_FOR_MONITOR/alertPolicies/$FRONTEND_ALERT_POLICY_ID"
          fi
          echo "Full Alert Policy ID for import: $FULL_ALERT_POLICY_ID"

          cat > main.tf << EOF
          provider "google" {
            project = "$GCP_PROJECT_ID_FOR_MONITOR"
            region  = "${{ env.REGION }}"
          }

          variable "target_hostname" { type = string }
          variable "gcp_project_id" { type = string }
          variable "naming_prefix" { type = string }
          variable "environment" { type = string }
          variable "alert_notification_channel_full_id" { type = string }
          variable "frontend_startup_alert_delay" { type = string }
          variable "alert_alignment_period" { type = string }

          resource "google_monitoring_uptime_check_config" "frontend_availability" {
            project      = var.gcp_project_id
            display_name = "\${var.naming_prefix}-frontend-availability-\${var.environment}"
            timeout      = "10s"
            period       = "300s" # Consistent with design_doc, adjust if var.frontend_uptime_check_period is available & preferred

            http_check {
              path         = "/"
              port         = "443"
              use_ssl      = true
              validate_ssl = true
              host         = var.target_hostname # This is the key dynamic part
            }

            content_matchers {
              content = "<title>Concept Visualizer</title>"
              matcher = "CONTAINS_STRING"
            }

            monitored_resource {
              type = "uptime_url"
              labels = {
                project_id = var.gcp_project_id
                host       = var.target_hostname # Also dynamic
              }
            }

            lifecycle {
              create_before_destroy = true
              # No ignore_changes here as this TF apply should be authoritative for these fields during workflow run
            }
          }

          resource "google_monitoring_alert_policy" "frontend_availability_failure_alert" {
            project      = var.gcp_project_id
            display_name = "\${var.naming_prefix}-frontend-down-al-\${var.environment}"
            combiner     = "OR"

            conditions {
              display_name = "Frontend Application Unavailable (Sustained)"
              condition_threshold {
                filter = "metric.type=\"monitoring.googleapis.com/uptime_check/check_passed\" AND resource.type=\"uptime_url\" AND metric.label.check_id=\"\${google_monitoring_uptime_check_config.frontend_availability.uptime_check_id}\""
                aggregations {
                  alignment_period   = var.alert_alignment_period
                  per_series_aligner = "ALIGN_FRACTION_TRUE"
                }
                comparison      = "COMPARISON_LT"
                threshold_value = 0.9
                duration        = var.frontend_startup_alert_delay
                trigger { count = 1 }
              }
            }

            notification_channels = [
              var.alert_notification_channel_full_id,
            ]

            documentation {
              content = <<EOT
          ### Frontend Availability Alert (\${var.environment})

          **Summary:** The frontend application is failing uptime checks.
          **Monitored Host:** \${var.target_hostname}
          **Uptime Check ID:** \${google_monitoring_uptime_check_config.frontend_availability.uptime_check_id}
          **Condition:** Success rate less than 90% over a \${var.alert_alignment_period} window, persisting for \${var.frontend_startup_alert_delay}.

          **Possible Causes:**
          *   Vercel platform issues.
          *   DNS resolution problems for your custom domain (if any).
          *   Misconfiguration in Vercel deployment settings.
          *   The application is not serving the expected content.
          *   The Uptime Check\'s target hostname in GCP Monitoring is outdated.

          **Recommended Actions:**
          1.  **Check Vercel Status:** Visit [status.vercel.com](https://status.vercel.com/).
          2.  **Check Vercel Deployment Logs.**
          3.  **Verify DNS Configuration.**
          4.  **Verify Uptime Check Target in GCP Monitoring.**
          5.  **Manually Access Frontend URL.**
          EOT
              mime_type = "text/markdown"
            }

            user_labels = {
              environment = var.environment
              service     = "\${var.naming_prefix}-frontend"
              tier        = "frontend"
            }
          }

          # Outputs for storing definitive IDs
          output "frontend_availability_config_full_id" {
            description = "Full ID of the frontend uptime check config."
            value = google_monitoring_uptime_check_config.frontend_availability.name
          }

          output "frontend_availability_alert_policy_full_id" {
            description = "Full ID of the frontend alert policy."
            value = google_monitoring_alert_policy.frontend_availability_failure_alert.name
          }
          EOF

          cat > terraform.tfvars <<EOF
          target_hostname                  = "$HOSTNAME"
          gcp_project_id                   = "$GCP_PROJECT_ID_FOR_MONITOR"
          naming_prefix                    = "${{ env.NAMING_PREFIX }}"
          environment                      = "${{ env.ENVIRONMENT }}"
          alert_notification_channel_full_id = "$ALERT_NOTIFICATION_CHANNEL_FULL_ID"
          frontend_startup_alert_delay     = "${{ env.FRONTEND_STARTUP_ALERT_DELAY }}"
          alert_alignment_period           = "${{ env.ALERT_ALIGNMENT_PERIOD }}"
          EOF

          terraform init -no-color

          echo "Importing Uptime Check: projects/$GCP_PROJECT_ID_FOR_MONITOR/uptimeCheckConfigs/$UPTIME_CHECK_ID_FOR_MONITOR into ephemeral TF state..."
          # Using actual resource name frontend_availability
          terraform import -no-color google_monitoring_uptime_check_config.frontend_availability "projects/$GCP_PROJECT_ID_FOR_MONITOR/uptimeCheckConfigs/$UPTIME_CHECK_ID_FOR_MONITOR" || echo "Uptime check import failed or resource does not exist. Continuing..."

          echo "Importing Alert Policy: $FULL_ALERT_POLICY_ID into ephemeral TF state..."
          # Using actual resource name frontend_availability_failure_alert
          terraform import -no-color google_monitoring_alert_policy.frontend_availability_failure_alert "$FULL_ALERT_POLICY_ID" || echo "Alert policy import failed or resource does not exist. Continuing..."

          terraform apply -auto-approve -no-color -var-file=terraform.tfvars

          if [ $? -eq 0 ]; then
            echo "Successfully updated frontend uptime check and alert policy via Terraform."

            # --- BEGIN: Store Definitive Resource IDs to GCS ---
            echo "Storing definitive resource IDs to GCS..."
            DEFINITIVE_UPTIME_CHECK_FULL_ID=$(terraform output -raw frontend_availability_config_full_id)
            DEFINITIVE_ALERT_POLICY_FULL_ID=$(terraform output -raw frontend_availability_alert_policy_full_id)

            if [[ -z "$DEFINITIVE_UPTIME_CHECK_FULL_ID" || -z "$DEFINITIVE_ALERT_POLICY_FULL_ID" ]]; then
              echo "ERROR: Could not retrieve definitive IDs from Terraform output. Skipping GCS storage."
            elif [[ -z "$TF_STATE_BUCKET_NAME" ]]; then
              echo "ERROR: TF_STATE_BUCKET_NAME is not set. Skipping GCS storage of IDs."
            else
              UPTIME_ID_GCS_PATH="gs://${TF_STATE_BUCKET_NAME}/dynamic_frontend_monitoring_ids/${{ env.ENVIRONMENT }}/frontend_uptime_check_id.txt"
              ALERT_POLICY_ID_GCS_PATH="gs://${TF_STATE_BUCKET_NAME}/dynamic_frontend_monitoring_ids/${{ env.ENVIRONMENT }}/frontend_alert_policy_id.txt"

              echo "Uptime Check ID to store: $DEFINITIVE_UPTIME_CHECK_FULL_ID at $UPTIME_ID_GCS_PATH"
              echo "$DEFINITIVE_UPTIME_CHECK_FULL_ID" | gsutil cp - "$UPTIME_ID_GCS_PATH"

              echo "Alert Policy ID to store: $DEFINITIVE_ALERT_POLICY_FULL_ID at $ALERT_POLICY_ID_GCS_PATH"
              echo "$DEFINITIVE_ALERT_POLICY_FULL_ID" | gsutil cp - "$ALERT_POLICY_ID_GCS_PATH"

              echo "Definitive IDs stored in GCS."
            fi
            # --- END: Store Definitive Resource IDs to GCS ---
          else
            echo "ERROR: Terraform apply failed. Review logs."
            exit 1
          fi
